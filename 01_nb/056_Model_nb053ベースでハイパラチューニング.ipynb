{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 056_Model_nb053ベースでハイパラチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import yaml\n",
    "import warnings\n",
    "import random\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import hashlib\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# usual\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import re\n",
    "\n",
    "# preprocess\n",
    "from fasttext import load_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "import texthero as hero\n",
    "import nltk\n",
    "import cv2\n",
    "from gensim.models import word2vec, KeyedVectors\n",
    "\n",
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "#import optuna.integration.lightgbm as lgb  # チューニング用\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from pandas_profiling import ProfileReport  # profile report を作る用\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# plot settings\n",
    "plt.rcParams[\"patch.force_edgecolor\"] = False\n",
    "plt.rcParams['font.family'] = 'sans_serif'\n",
    "sns.set(style=\"whitegrid\",  palette=\"muted\", color_codes=True, rc={'grid.linestyle': '--'})\n",
    "red = sns.xkcd_rgb[\"light red\"]\n",
    "green = sns.xkcd_rgb[\"medium green\"]\n",
    "blue = sns.xkcd_rgb[\"denim blue\"]\n",
    "\n",
    "# plot extentions\n",
    "import japanize_matplotlib\n",
    "from matplotlib_venn import venn2\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed固定\n",
    "def set_seed(seed=2021):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "SEED = 2021\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nltk.download('stopwords')\n",
    "os.listdir(os.path.expanduser('~/nltk_data/corpora/stopwords/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 試験ID生成\n",
    "trial_prefix = 'nb056'  # ←手動で指定 \n",
    "dttm_now = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "trial_id = f'{trial_prefix}_{dttm_now}'\n",
    "\n",
    "print(trial_prefix)\n",
    "print(trial_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アウトプット先指定\n",
    "OUTPUT_DIR = Path(f\"../02_outputs/{trial_prefix}\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データのパス指定\n",
    "DATA_DIR = '../00_input/atmacup10_dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 処理実行フラグ\n",
    "RUN_W2V = True\n",
    "#RUN_TFIDF = True\n",
    "RUN_FEAT_SELECTION = True\n",
    "#RUN_FEAT_SELECTION_IMPORTANCE = False\n",
    "RUN_OPTUNA = True\n",
    "\n",
    "# 特徴出力先指定\n",
    "FEAT_DIR = Path(f\"../03_feature/\")\n",
    "\n",
    "# 入出力のnotebook番号\n",
    "feat_input = 'nb036'\n",
    "feat_output = trial_prefix\n",
    "\n",
    "feat_selection_input = 'nb042'\n",
    "feat_selection_output = trial_prefix\n",
    "\n",
    "tuned_model_input = 'nb042'\n",
    "tuned_model_output = trial_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test\n",
    "train_base = pd.read_csv(DATA_DIR + 'train.csv')\n",
    "test_base = pd.read_csv(DATA_DIR + 'test.csv')\n",
    "\n",
    "# 絵画の属性(object_idでJOIN)\n",
    "color = pd.read_csv(DATA_DIR + 'color.csv')\n",
    "palette = pd.read_csv(DATA_DIR + 'palette.csv')\n",
    "material = pd.read_csv(DATA_DIR + 'material.csv')\n",
    "historical_person = pd.read_csv(DATA_DIR + 'historical_person.csv')\n",
    "object_collection = pd.read_csv(DATA_DIR + 'object_collection.csv')\n",
    "production_place = pd.read_csv(DATA_DIR + 'production_place.csv')\n",
    "technique = pd.read_csv(DATA_DIR + 'technique.csv')\n",
    "\n",
    "# 作家の情報(nameでJOIN)\n",
    "maker = pd.read_csv(DATA_DIR + 'maker.csv')\n",
    "\n",
    "# 作品に関係した作家(vs作品情報:object_id, vs作家情報:name)\n",
    "principal_maker = pd.read_csv(DATA_DIR + 'principal_maker.csv')\n",
    "\n",
    "# 作家が作品にどうか変わったか(ちょっと扱いがわからない。保留)\n",
    "principal_maker_occupation = pd.read_csv(DATA_DIR + 'principal_maker_occupation.csv')\n",
    "\n",
    "# sample_submission\n",
    "atmacup10__sample_submission = pd.read_csv(DATA_DIR + 'atmacup10__sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 事前確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 特徴作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_table(input_df, right_df, left_on, right_on, right_prefix=None):\n",
    "    join_df = right_df.add_prefix(right_prefix)\n",
    "    out_df = input_df.merge(join_df, left_on=left_on, right_on=right_prefix+right_on, how='left')\n",
    "    out_df.drop(right_prefix+right_on, axis=1, inplace=True)\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = merge_table(train_base, maker, left_on='principal_maker', right_on='name', right_prefix='principal_maker_')\n",
    "test  = merge_table(test_base, maker, left_on='principal_maker', right_on='name', right_prefix='principal_maker_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "train.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.shape)\n",
    "test.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理＆特徴量作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴出力用カラム指定＆作成\n",
    "FEAT_W2V_DIR_IN = FEAT_DIR/f'w2v/{feat_input}'\n",
    "FEAT_W2V_DIR_OUT = FEAT_DIR/f'w2v/{feat_output}'\n",
    "FEAT_W2V_DIR_OUT.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess: color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.guruguru.science/competitions/16/discussions/a69d0d70-3e50-4eb7-9d65-22bc87e380af/\n",
    "import pandas as pd\n",
    "from PIL import ImageColor\n",
    "\n",
    "color_rgb = pd.DataFrame(color['hex'].str.strip().map(ImageColor.getrgb)\\\n",
    "                         .values.tolist(), columns=['color_r', 'color_g', 'color_b'])\n",
    "color_rgb_df = pd.concat([color, color_rgb], axis=1).drop('hex', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys\n",
    "\n",
    "def rgb_encoder(input_df:pd.DataFrame(), idcols:list, prefix:str):\n",
    "    tmp_ = input_df.copy()\n",
    "    scale_csys = pd.DataFrame()\n",
    "\n",
    "    # colorsysを用いた変換\n",
    "    tmp_['hsv'] = tmp_[['color_r', 'color_g', 'color_b']]\\\n",
    "                        .apply(lambda x: colorsys.rgb_to_hsv(x[0], x[1],x[2]), axis=1)\n",
    "\n",
    "    tmp_['yiq'] = tmp_[['color_r', 'color_g', 'color_b']]\\\n",
    "                        .apply(lambda x: colorsys.rgb_to_yiq(x[0], x[1],x[2]), axis=1)\n",
    "    tmp_['hls'] = tmp_[['color_r', 'color_g', 'color_b']]\\\n",
    "                        .apply(lambda x: colorsys.rgb_to_hls(x[0], x[1],x[2]), axis=1)\n",
    "\n",
    "    scale_csys['hsv_h'] = tmp_['hsv'].map(lambda x: x[0])\n",
    "    scale_csys['hsv_s'] = tmp_['hsv'].map(lambda x: x[1])\n",
    "    scale_csys['hsv_v'] = tmp_['hsv'].map(lambda x: x[2])\n",
    "\n",
    "    scale_csys['yiq_y'] = tmp_['yiq'].map(lambda x: x[0])\n",
    "    scale_csys['yiq_i'] = tmp_['yiq'].map(lambda x: x[1])\n",
    "    scale_csys['yiq_q'] = tmp_['yiq'].map(lambda x: x[2])\n",
    "\n",
    "    scale_csys['hls_h'] = tmp_['hls'].map(lambda x: x[0])\n",
    "    scale_csys['hls_l'] = tmp_['hls'].map(lambda x: x[1])\n",
    "    scale_csys['hls_s'] = tmp_['hls'].map(lambda x: x[2])\n",
    "\n",
    "    # opencvを用いた変換\n",
    "    rgb_ = np.array(tmp_[['color_r', 'color_g', 'color_b']]) #RGB\n",
    "    rgb_ = rgb_[:, np.newaxis,:] # 画像の行列形状に\n",
    "    rgb_ = rgb_.astype(np.uint8) \n",
    "    \n",
    "    lab = cv2.cvtColor(rgb_, cv2.COLOR_RGB2Lab)\n",
    "    lab_df = pd.DataFrame(lab[:,0,:], columns=['lab_L', 'lab_a', 'lab_b'])\n",
    "    ycrcb = cv2.cvtColor(rgb_, cv2.COLOR_BGR2YCrCb)\n",
    "    ycrcb_df = pd.DataFrame(lab[:,0,:], columns=['YCrCb_Y', 'YCrCb_Cr', 'YCrCb_Cb'])\n",
    "    lub = cv2.cvtColor(rgb_, cv2.COLOR_RGB2Luv)\n",
    "    lub_df = pd.DataFrame(lab[:,0,:], columns=['Luv_L', 'Luv_u', 'Luv_v'])\n",
    "    xyz = cv2.cvtColor(rgb_, cv2.COLOR_RGB2XYZ)\n",
    "    xyz_df = pd.DataFrame(lab[:,0,:], columns=['XYZ_X', 'XYZ_Y', 'XYZ_Z'])\n",
    "    gray = cv2.cvtColor(rgb_, cv2.COLOR_RGB2GRAY)\n",
    "    gray_df = pd.DataFrame(gray)\n",
    "    gray_df.columns = ['grayscale']\n",
    "\n",
    "    scale_cv = pd.concat([lab_df, lub_df, xyz_df, gray_df], axis=1)\n",
    "\n",
    "    color_scales = pd.concat([scale_csys, scale_cv], axis=1).add_prefix(f'{prefix}')\n",
    "    color_scales_cols = color_scales.columns.tolist()\n",
    "\n",
    "    out_df = pd.concat([tmp_[idcols], color_scales], axis=1)\n",
    "    \n",
    "    return out_df, color_scales_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_df, color_cols = rgb_encoder(color_rgb_df, ['object_id', 'percentage'], 'color_')\n",
    "print(color_cols)\n",
    "color_df['ratio'] = color_df['percentage'] * 0.01\n",
    "color_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_transform(input_df:pd.DataFrame(), color_df:pd.DataFrame(), tgcols):\n",
    "    output_df = input_df[['object_id']].copy()\n",
    "    color_tmp = color_df.copy()\n",
    "\n",
    "    # 平均を取得\n",
    "    mean_palette = color_tmp.copy()\n",
    "    for col_ in tgcols:\n",
    "        mean_palette[f'mean_{col_}'] = mean_palette['ratio'] * mean_palette[col_]\n",
    "    print(mean_palette.columns)\n",
    "    mean_group = mean_palette.groupby('object_id')[[f'mean_{i}' for i in tgcols]].sum().reset_index()\n",
    "    output_df = pd.merge(output_df, mean_group, on=\"object_id\", how=\"left\")\n",
    "    print(output_df.columns)\n",
    "\n",
    "    # 分散を取得\n",
    "    var_palette_grp = color_tmp.groupby(\"object_id\")[tgcols\n",
    "                                                      ].var().add_prefix('var_').reset_index()\n",
    "    output_df = pd.merge(output_df, var_palette_grp, on=\"object_id\", how=\"left\")\n",
    "    print(output_df.columns)\n",
    "\n",
    "    # 標準偏差を取得\n",
    "    var_palette_grp = color_tmp.groupby(\"object_id\")[tgcols\n",
    "                                                      ].std().add_prefix('std_').reset_index()\n",
    "    output_df = pd.merge(output_df, var_palette_grp, on=\"object_id\", how=\"left\")\n",
    "    print(output_df.columns)\n",
    "\n",
    "    # ratioの分散を取得\n",
    "    var_ratio = color_tmp.groupby('object_id')['ratio'].var()\n",
    "    output_df['var_color_ratio'] = output_df['object_id'].map(var_ratio)\n",
    "\n",
    "    return output_df.drop(['object_id'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_train = color_transform(train, color_df, color_cols)\n",
    "color_test = color_transform(test, color_df, color_cols)\n",
    "\n",
    "color_columns = color_train.columns.tolist()\n",
    "\n",
    "train = pd.concat([train, color_train], axis=1)\n",
    "test = pd.concat([test, color_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess: palette_hsv_yiq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys\n",
    "\n",
    "palette_hsv_yiq = palette.copy()\n",
    "palette_hsv_yiq['hsv'] = palette_hsv_yiq[['color_r', 'color_g', 'color_b']]\\\n",
    "                    .apply(lambda x: colorsys.rgb_to_hsv(x[0], x[1],x[2]), axis=1)\n",
    "\n",
    "palette_hsv_yiq['yiq'] = palette_hsv_yiq[['color_r', 'color_g', 'color_b']]\\\n",
    "                    .apply(lambda x: colorsys.rgb_to_yiq(x[0], x[1],x[2]), axis=1)\n",
    "palette_hsv_yiq['hls'] = palette_hsv_yiq[['color_r', 'color_g', 'color_b']]\\\n",
    "                    .apply(lambda x: colorsys.rgb_to_hls(x[0], x[1],x[2]), axis=1)\n",
    "\n",
    "palette_hsv_yiq['hsv_h'] = palette_hsv_yiq['hsv'].map(lambda x: x[0])\n",
    "palette_hsv_yiq['hsv_s'] = palette_hsv_yiq['hsv'].map(lambda x: x[1])\n",
    "palette_hsv_yiq['hsv_v'] = palette_hsv_yiq['hsv'].map(lambda x: x[2])\n",
    "\n",
    "palette_hsv_yiq['yiq_y'] = palette_hsv_yiq['yiq'].map(lambda x: x[0])\n",
    "palette_hsv_yiq['yiq_i'] = palette_hsv_yiq['yiq'].map(lambda x: x[1])\n",
    "palette_hsv_yiq['yiq_q'] = palette_hsv_yiq['yiq'].map(lambda x: x[2])\n",
    "\n",
    "palette_hsv_yiq['hls_h'] = palette_hsv_yiq['hls'].map(lambda x: x[0])\n",
    "palette_hsv_yiq['hls_l'] = palette_hsv_yiq['hls'].map(lambda x: x[1])\n",
    "palette_hsv_yiq['hls_s'] = palette_hsv_yiq['hls'].map(lambda x: x[2])\n",
    "\n",
    "\n",
    "color_sys_cols = ['hsv_h','hsv_s','hsv_v',\n",
    "                          'yiq_y','yiq_i','yiq_q'\n",
    "                          #'hls_h','hls_l','hls_s'\n",
    "                 ]\n",
    "\n",
    "palette_hsv_yiq = palette_hsv_yiq[['ratio',\n",
    "                          'object_id',\n",
    "                          *color_sys_cols]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_palette_hsv_yiq(input_df, tgcols=color_sys_cols):\n",
    "    output_df = input_df[['object_id']].copy()\n",
    "    palette_tmp = palette_hsv_yiq.copy()\n",
    "\n",
    "    # 平均のlabを取得\n",
    "    mean_palette = palette_tmp.copy()\n",
    "    for col_ in tgcols:\n",
    "        mean_palette[f'mean_{col_}'] = mean_palette['ratio'] * mean_palette[col_]\n",
    "    print(mean_palette.columns)\n",
    "    mean_group = mean_palette.groupby('object_id')[[f'mean_{i}' for i in tgcols]].sum().reset_index()\n",
    "    output_df = pd.merge(output_df, mean_group, on=\"object_id\", how=\"left\")\n",
    "    print(output_df.columns)\n",
    "\n",
    "    # labの分散を取得\n",
    "    var_palette_grp = palette_tmp.groupby(\"object_id\")[tgcols\n",
    "                                                      ].var().add_prefix('var_').reset_index()\n",
    "    output_df = pd.merge(output_df, var_palette_grp, on=\"object_id\", how=\"left\")\n",
    "    print(output_df.columns)\n",
    "\n",
    "    # labの標準偏差を取得\n",
    "    var_palette_grp = palette_tmp.groupby(\"object_id\")[tgcols\n",
    "                                                      ].std().add_prefix('std_').reset_index()\n",
    "    output_df = pd.merge(output_df, var_palette_grp, on=\"object_id\", how=\"left\")\n",
    "    print(output_df.columns)\n",
    "\n",
    "    # ratioの分散を取得\n",
    "    #var_ratio = palette_lab.groupby('object_id')['ratio'].var()\n",
    "    #output_df['var_ratio'] = output_df['object_id'].map(var_ratio)\n",
    "\n",
    "    return output_df.drop(['object_id'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_palette_hsv_yiq(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette_train_hsvyiq = create_palette_hsv_yiq(train)\n",
    "palette_test_hsvyiq = create_palette_hsv_yiq(test)\n",
    "\n",
    "feat_palette_hsvyiq = palette_train_hsvyiq.columns.tolist()\n",
    "\n",
    "train = pd.concat([train, palette_train_hsvyiq], axis=1)\n",
    "test = pd.concat([test, palette_test_hsvyiq], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_palette_hsvyiq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette_train_hsvyiq.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess: palette_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_scale_opencv():\n",
    "    rgb_ = np.array(palette[['color_r', 'color_g', 'color_b']]) #RGB\n",
    "    rgb_ = rgb_[:, np.newaxis,:] # 画像の行列形状に\n",
    "    rgb_ = rgb_.astype(np.uint8) \n",
    "    \n",
    "    lab = cv2.cvtColor(rgb_, cv2.COLOR_RGB2Lab)\n",
    "    lab_df = pd.DataFrame(lab[:,0,:], columns=['lab_L', 'lab_a', 'lab_b'])\n",
    "    ycrcb = cv2.cvtColor(rgb_, cv2.COLOR_BGR2YCrCb)\n",
    "    ycrcb_df = pd.DataFrame(lab[:,0,:], columns=['YCrCb_Y', 'YCrCb_Cr', 'YCrCb_Cb'])\n",
    "    lub = cv2.cvtColor(rgb_, cv2.COLOR_RGB2Luv)\n",
    "    lub_df = pd.DataFrame(lab[:,0,:], columns=['Luv_L', 'Luv_u', 'Luv_v'])\n",
    "    xyz = cv2.cvtColor(rgb_, cv2.COLOR_RGB2XYZ)\n",
    "    xyz_df = pd.DataFrame(lab[:,0,:], columns=['XYZ_X', 'XYZ_Y', 'XYZ_Z'])\n",
    "    gray = cv2.cvtColor(rgb_, cv2.COLOR_RGB2GRAY)\n",
    "    gray_df = pd.DataFrame(gray)\n",
    "    gray_df.columns = ['grayscale']\n",
    "\n",
    "    palette_cv = pd.concat([lab_df, lub_df, xyz_df, gray_df], axis=1)\n",
    "    palette_cv_cols = palette_cv.columns.tolist()\n",
    "    palette_out = pd.concat([palette, palette_cv], axis=1).drop(['color_r', 'color_g', 'color_b'], axis=1)\n",
    "\n",
    "    return palette_out, palette_cv_cols\n",
    "\n",
    "\n",
    "def create_palette_feature_opencv(input_df, palette_df, tgcols):\n",
    "    output_df = input_df[['object_id']].copy()\n",
    "\n",
    "    # ratio最大のものを取得\n",
    "    max_palette = palette_df.groupby('object_id')['ratio'].max().reset_index()\n",
    "    max_palette = pd.merge(max_palette,\n",
    "                           palette_df,\n",
    "                           on=['object_id','ratio'],\n",
    "                           how='left'\n",
    "                          ).set_index(['object_id','ratio']\n",
    "                                     ).add_prefix('max_').reset_index()\n",
    "    \n",
    "    max_palette = max_palette.loc[\n",
    "                            max_palette[\"object_id\"].drop_duplicates().index.tolist()\n",
    "                            ].reset_index()  # 同じidでmax ratioが同じものは削除\n",
    "\n",
    "    output_df = pd.merge(output_df, max_palette, on=\"object_id\", how=\"left\")\n",
    "    print(output_df.columns)\n",
    "\n",
    "    # 平均を取得\n",
    "    mean_palette = palette_df.copy()\n",
    "    for col_ in tgcols:\n",
    "        mean_palette[f'mean_{col_}'] = mean_palette['ratio'] * mean_palette[col_]\n",
    "    print(mean_palette.columns)\n",
    "    mean_group = mean_palette.groupby('object_id')[[f'mean_{i}' for i in tgcols]].sum().reset_index()\n",
    "    output_df = pd.merge(output_df, mean_group, on=\"object_id\", how=\"left\")\n",
    "    print(output_df.columns)\n",
    "\n",
    "    # 分散を取得\n",
    "    var_palette_grp = palette_df.groupby(\"object_id\")[tgcols\n",
    "                                                      ].var().add_prefix('var_').reset_index()\n",
    "    output_df = pd.merge(output_df, var_palette_grp, on=\"object_id\", how=\"left\")\n",
    "    print(output_df.columns)\n",
    "\n",
    "    # 標準偏差を取得\n",
    "    var_palette_grp = palette_df.groupby(\"object_id\")[tgcols\n",
    "                                                      ].std().add_prefix('std_').reset_index()\n",
    "    output_df = pd.merge(output_df, var_palette_grp, on=\"object_id\", how=\"left\")\n",
    "    print(output_df.columns)\n",
    "\n",
    "    # ratioの分散を取得\n",
    "    #var_ratio = palette_lab.groupby('object_id')['ratio'].var()\n",
    "    #output_df['var_ratio'] = output_df['object_id'].map(var_ratio)\n",
    "\n",
    "    return output_df.drop(['index', 'object_id'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette_cv, palette_cv_cols = rgb_to_scale_opencv()\n",
    "\n",
    "palette_train = create_palette_feature_opencv(train, palette_cv, palette_cv_cols)\n",
    "palette_test = create_palette_feature_opencv(test, palette_cv, palette_cv_cols)\n",
    "\n",
    "feat_palette_cv = palette_train.columns.tolist()\n",
    "\n",
    "train = pd.concat([train, palette_train], axis=1)\n",
    "test = pd.concat([test, palette_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess: W2V_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語ベクトル表現の次元数\n",
    "# 元の語彙数をベースに適当に決めました\n",
    "model_size = {\n",
    "    \"material\": 20,\n",
    "    \"technique\": 8,\n",
    "    \"collection\": 3,\n",
    "    \"material_collection\": 20,\n",
    "    \"material_technique\": 20,\n",
    "    \"collection_technique\": 10,\n",
    "    \"material_collection_technique\": 25,\n",
    "    \"historical_person\": 30, \n",
    "    \"production_place\": 20,\n",
    "    \"historical_person__production_place\":50 \n",
    "}\n",
    "\n",
    "n_iter = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashfxn(x):\n",
    "    return int(hashlib.md5(str(x).encode()).hexdigest(), 16)\n",
    "\n",
    "def create_w2v(df_list:list, df_nm_list:list):\n",
    "    w2v_dfs = []\n",
    "    w2v_cols = []\n",
    "    for df, df_name in zip(df_list, df_nm_list):\n",
    "        df_group = df.groupby(\"object_id\")[\"name\"].apply(list).reset_index()\n",
    "        # Word2Vecの学習\n",
    "        w2v_model = word2vec.Word2Vec(df_group[\"name\"].values.tolist(),\n",
    "                                      size=model_size[df_name],\n",
    "                                      min_count=1,\n",
    "                                      window=1,\n",
    "                                      seed=SEED,\n",
    "                                      workers=1,\n",
    "                                      hashfxn=hashfxn,\n",
    "                                      iter=n_iter)\n",
    "\n",
    "        # 各文章ごとにそれぞれの単語をベクトル表現に直し、平均をとって文章ベクトルにする\n",
    "        sentence_vectors = df_group[\"name\"].progress_apply(\n",
    "            lambda x: np.mean([w2v_model.wv[e] for e in x], axis=0))\n",
    "        sentence_vectors = np.vstack([x for x in sentence_vectors])\n",
    "        sentence_vector_df = pd.DataFrame(sentence_vectors,\n",
    "                                          columns=[f\"{df_name}_w2v_{i}\"\n",
    "                                                   for i in range(model_size[df_name])])\n",
    "        sentence_vector_df.index = df_group[\"object_id\"]\n",
    "        w2v_dfs.append(sentence_vector_df)\n",
    "        w2v_cols += sentence_vector_df.columns.tolist()\n",
    "    return w2v_dfs, w2v_cols\n",
    "\n",
    "\n",
    "def add_w2v_dfs(input_df:pd.DataFrame, w2v_dfs:list):\n",
    "    out_df = input_df.copy()\n",
    "    for _df in w2v_dfs:\n",
    "        _df = _df.reset_index()\n",
    "        out_df = pd.merge(out_df, _df, on='object_id', how='left')\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_W2V == True:\n",
    "    mat_col = pd.concat([material, object_collection], axis=0).reset_index(drop=True)\n",
    "    mat_tec = pd.concat([material, technique], axis=0).reset_index(drop=True)\n",
    "    col_tec = pd.concat([object_collection, technique], axis=0).reset_index(drop=True)\n",
    "    mat_col_tec = pd.concat([material, object_collection, technique], axis=0).reset_index(drop=True)\n",
    "    per_plc = pd.concat([historical_person, production_place, technique], axis=0).reset_index(drop=True)\n",
    "\n",
    "    df_list = [material, object_collection, technique, mat_col, mat_tec, col_tec, mat_col_tec,\n",
    "              historical_person, production_place, per_plc]\n",
    "    df_nm_list = [\n",
    "                \"material\", \"collection\", \"technique\",\n",
    "                \"material_collection\",\n",
    "                \"material_technique\",\n",
    "                \"collection_technique\",\n",
    "                \"material_collection_technique\",\n",
    "                \"historical_person\", \"production_place\",\n",
    "                \"historical_person__production_place\"\n",
    "            ]\n",
    "\n",
    "    w2v_dfs, w2v_cols = create_w2v(df_list, df_nm_list)\n",
    "\n",
    "    with open(FEAT_W2V_DIR_OUT/f'w2v_dfs.pkl', 'wb') as f:\n",
    "        pickle.dump(w2v_dfs , f)\n",
    "\n",
    "    with open(FEAT_W2V_DIR_OUT/f'w2v_cols.pkl', 'wb') as f:\n",
    "        pickle.dump(w2v_cols , f)\n",
    "\n",
    "else:    \n",
    "    with open(FEAT_W2V_DIR_IN/f'w2v_dfs.pkl', 'rb') as f:\n",
    "        w2v_dfs = pickle.load(f)\n",
    "\n",
    "    with open(FEAT_W2V_DIR_IN/f'w2v_cols.pkl', 'rb') as f:\n",
    "        w2v_cols = pickle.load(f)\n",
    "\n",
    "train = add_w2v_dfs(train, w2v_dfs)\n",
    "test = add_w2v_dfs(test, w2v_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess: W2V_Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = {\n",
    "    \"title\": 30,\n",
    "    \"description\": 30,\n",
    "    \"more_title\": 15,\n",
    "    \"long_title\": 15\n",
    "    ,'acquisition_credit_line':15\n",
    "}\n",
    "\n",
    "text_cols = ['title', 'description', 'more_title', 'long_title', 'acquisition_credit_line']\n",
    "\n",
    "n_iter = 100\n",
    "\n",
    "def text_normalization(text):\n",
    "\n",
    "    # 英語とオランダ語を stopword として指定\n",
    "    custom_stopwords = nltk.corpus.stopwords.words('dutch') + nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    x = hero.clean(text, pipeline=[\n",
    "        hero.preprocessing.fillna,\n",
    "        hero.preprocessing.lowercase,\n",
    "        hero.preprocessing.remove_digits,\n",
    "        hero.preprocessing.remove_punctuation,\n",
    "        hero.preprocessing.remove_diacritics,\n",
    "        lambda x: hero.preprocessing.remove_stopwords(x, stopwords=custom_stopwords)\n",
    "    ])\n",
    "\n",
    "    return x\n",
    "\n",
    "def hashfxn(x):\n",
    "    return int(hashlib.md5(str(x).encode()).hexdigest(), 16)\n",
    "\n",
    "def create_w2v_single_table(input_df:pd.DataFrame, columns:list):\n",
    "    w2v_dfs = []\n",
    "    w2v_cols = []\n",
    "    for col_ in columns:\n",
    "        tmp_df = input_df[['object_id', col_]].copy()\n",
    "        tmp_df = tmp_df.dropna(axis=0) \n",
    "        tmp_df['name'] = text_normalization(tmp_df[col_]).\\\n",
    "                            map(lambda x: [i for i in x.split(' ') if i not in (' ')])  \n",
    "\n",
    "        # Word2Vecの学習\n",
    "        w2v_model = word2vec.Word2Vec(tmp_df['name'].values.tolist(),\n",
    "                                      size=model_size[col_],\n",
    "                                      min_count=1,\n",
    "                                      window=5,\n",
    "                                      seed=SEED,\n",
    "                                      workers=1,\n",
    "                                      hashfxn=hashfxn,\n",
    "                                      iter=n_iter)\n",
    "\n",
    "        # 各文章ごとにそれぞれの単語をベクトル表現に直し、平均をとって文章ベクトルにする\n",
    "        sentence_vectors = tmp_df['name'].progress_apply(\n",
    "            lambda x: np.mean([w2v_model.wv[e] for e in x], axis=0))\n",
    "        sentence_vectors = np.vstack([x for x in sentence_vectors])\n",
    "        sentence_vector_df = pd.DataFrame(sentence_vectors,\n",
    "                                          columns=[f\"{col_}_w2v_{i}\"\n",
    "                                                   for i in range(model_size[col_])])\n",
    "        sentence_vector_df.index = tmp_df[\"object_id\"]\n",
    "        w2v_dfs.append(sentence_vector_df)\n",
    "        w2v_cols += sentence_vector_df.columns.tolist()\n",
    "    return w2v_dfs, w2v_cols\n",
    "\n",
    "\n",
    "def add_w2v_dfs(input_df:pd.DataFrame, w2v_dfs:list):\n",
    "    out_df = input_df.copy()\n",
    "    for _df in w2v_dfs:\n",
    "        _df = _df.reset_index()\n",
    "        out_df = pd.merge(out_df, _df, on='object_id', how='left')\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_W2V == True:\n",
    "    whole_df = pd.concat([train_base, test_base])\n",
    "    w2v_dfs_text, w2v_cols_text = create_w2v_single_table(whole_df, text_cols)\n",
    "\n",
    "    with open(FEAT_W2V_DIR_OUT/f'w2v_dfs_text.pkl', 'wb') as f:\n",
    "        pickle.dump(w2v_dfs_text , f)\n",
    "\n",
    "    with open(FEAT_W2V_DIR_OUT/f'w2v_cols_text.pkl', 'wb') as f:\n",
    "        pickle.dump(w2v_cols_text , f)\n",
    "    \n",
    "else:\n",
    "    with open(FEAT_W2V_DIR_IN/f'w2v_dfs_text.pkl', 'rb') as f:\n",
    "        w2v_dfs_text = pickle.load(f)\n",
    "\n",
    "    with open(FEAT_W2V_DIR_IN/f'w2v_cols_text.pkl', 'rb') as f:\n",
    "        w2v_cols_text = pickle.load(f)\n",
    "\n",
    "train = add_w2v_dfs(train, w2v_dfs_text)\n",
    "test = add_w2v_dfs(test, w2v_dfs_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: date_transform\n",
    "- 日付系の変換\n",
    "- acquisition_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquisition_dateの変換\n",
    "def acquisition_date_transform(input_df:pd.DataFrame, tg_col='acquisition_date'):\n",
    "    out_df = pd.DataFrame()\n",
    "    out_df[f'{tg_col}_year'] = input_df[tg_col]\\\n",
    "                                .fillna('nan')\\\n",
    "                                .map(lambda x: x[:4] if x!='nan' else np.nan)\\\n",
    "                                .astype('float')\n",
    "    out_df[f'{tg_col}_month'] = input_df[tg_col]\\\n",
    "                                .fillna('nan')\\\n",
    "                                .map(lambda x: x[5:7] if x!='nan' else np.nan)\\\n",
    "                                .astype('float')\n",
    "    out_df[f'{tg_col}_day'] = input_df[tg_col]\\\n",
    "                                .fillna('nan')\\\n",
    "                                .map(lambda x: x[8:10] if x!='nan' else np.nan)\\\n",
    "                                .astype('float')\n",
    "    out_df[f'{tg_col}_ym'] = input_df[tg_col]\\\n",
    "                                .fillna('nan')\\\n",
    "                                .map(lambda x: x[:4]+x[5:7] if x!='nan' else np.nan)\\\n",
    "                                .astype('float')\n",
    "    out_df[f'{tg_col}_ymd'] = input_df[tg_col]\\\n",
    "                                .fillna('nan')\\\n",
    "                                .map(lambda x: x[:4]+x[5:7]+x[8:10] if x!='nan' else np.nan)\\\n",
    "                                .astype('float')\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, acquisition_date_transform(train)], axis=1)\n",
    "test = pd.concat([test, acquisition_date_transform(test)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: sub_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_title_extract(input_df:pd.DataFrame):\n",
    "    out_df = pd.DataFrame()\n",
    "\n",
    "    for axis in ['h', 'w', 't', 'd']:\n",
    "        column_name = f'size_{axis}'\n",
    "        size_info = input_df['sub_title'].str.extract(r'{} (\\d*|\\d*\\.\\d*)(cm|mm)'.format(axis)) # 正規表現を使ってサイズを抽出\n",
    "        size_info = size_info.rename(columns={0: column_name, 1: 'unit'})\n",
    "        size_info[column_name] = size_info[column_name].replace('', np.nan).astype(float) # dtypeがobjectになってるのでfloatに直す\n",
    "        size_info[column_name] = size_info.apply(lambda row: row[column_name] * 10 if row['unit'] == 'cm' else row[column_name], axis=1) # 　単位をmmに統一する\n",
    "        out_df[column_name] = size_info[column_name] # trainにくっつける\n",
    "        \n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, sub_title_extract(train)], axis=1)\n",
    "test = pd.concat([test, sub_title_extract(test)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: diff\n",
    "- 差分\n",
    "- 掛け算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引き算\n",
    "def diff_features(input_df:pd.DataFrame, col_left:str, col_right:str, new_colname=None):\n",
    "    out_df = pd.DataFrame()\n",
    "    if new_colname == None:\n",
    "        new_colname = f'diff_{col_left}__{col_right}'\n",
    "    out_df[new_colname] = input_df[col_left] - input_df[col_right]        \n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 掛け算\n",
    "def products_feats(input_df:pd.DataFrame, col_left:str, col_right:str, new_colname=None):\n",
    "    out_df = pd.DataFrame()\n",
    "    if new_colname == None:\n",
    "        new_colname = f'products_{col_left}__{col_right}'\n",
    "    out_df[new_colname] = input_df[col_left] * input_df[col_right]        \n",
    "    return out_df\n",
    "\n",
    "# 割り算\n",
    "def division_feats(input_df:pd.DataFrame, col_left:str, col_right:str, new_colname=None):\n",
    "    out_df = pd.DataFrame()\n",
    "    if new_colname == None:\n",
    "        new_colname = f'products_{col_left}__{col_right}'\n",
    "    out_df[new_colname] = input_df[col_left] / input_df[col_right] \n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffを取る特徴を辞書としてリストに格納\n",
    "diff_dic_lst = [\n",
    "           {'col_l':'dating_year_late',\n",
    "            'col_r':'dating_year_early',\n",
    "            'new_colname':'dating_presenting_year_num'},\n",
    "           {'col_l':'acquisition_date_year',\n",
    "            'col_r':'dating_year_late',\n",
    "            'new_colname':None}\n",
    "]\n",
    "\n",
    "prd_dic_lst = [\n",
    "           {'col_l':'size_h',\n",
    "            'col_r':'size_w',\n",
    "            'new_colname':'size_square'}\n",
    "]\n",
    "\n",
    "div_dic_lst = [\n",
    "           {'col_l':'size_w',\n",
    "            'col_r':'size_h',\n",
    "            'new_colname':'size_aspect_ratio'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各組み合わせについて差分を取ったカラムを追加\n",
    "for dic_ in diff_dic_lst:\n",
    "    train = pd.concat([train,\n",
    "                       diff_features(train, dic_['col_l'] , dic_['col_r'] , dic_['new_colname'])],\n",
    "                       axis=1)\n",
    "    test = pd.concat([test,\n",
    "                      diff_features(test, dic_['col_l'] , dic_['col_r'] , dic_['new_colname'])],\n",
    "                      axis=1)\n",
    "    \n",
    "for dic_ in prd_dic_lst:\n",
    "    train = pd.concat([train,\n",
    "                       products_feats(train, dic_['col_l'] , dic_['col_r'] , dic_['new_colname'])],\n",
    "                       axis=1)\n",
    "    test = pd.concat([test,\n",
    "                      products_feats(test, dic_['col_l'] , dic_['col_r'] , dic_['new_colname'])],\n",
    "                      axis=1)\n",
    "    \n",
    "for dic_ in div_dic_lst:\n",
    "    train = pd.concat([train,\n",
    "                       division_feats(train, dic_['col_l'] , dic_['col_r'] , dic_['new_colname'])],\n",
    "                       axis=1)\n",
    "    test = pd.concat([test,\n",
    "                      division_feats(test, dic_['col_l'] , dic_['col_r'] , dic_['new_colname'])],\n",
    "                      axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"../99_bin/lid.176.bin\")\n",
    "def lang_transform(input_df:pd.DataFrame, tg_col:str):\n",
    "    out_df = pd.DataFrame()\n",
    "    out_df[f\"{tg_col}_lang_ft\"] = input_df[tg_col].fillna(\"\").map(\n",
    "        lambda x: model.predict(x.replace(\"\\n\", \"\"))[0][0])\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, lang_transform(train, 'title')], axis=1)\n",
    "test = pd.concat([test, lang_transform(test, 'title')], axis=1)\n",
    "\n",
    "train = pd.concat([train, lang_transform(train, 'long_title')], axis=1)\n",
    "test = pd.concat([test, lang_transform(test, 'long_title')], axis=1)\n",
    "\n",
    "train = pd.concat([train, lang_transform(train, 'more_title')], axis=1)\n",
    "test = pd.concat([test, lang_transform(test, 'more_title')], axis=1)\n",
    "\n",
    "train = pd.concat([train, lang_transform(train, 'description')], axis=1)\n",
    "test = pd.concat([test, lang_transform(test, 'description')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 関数化\n",
    "tg_techcol = ['technique_etching',\n",
    " 'technique_engraving',\n",
    " 'technique_albumen print',\n",
    " 'technique_gelatin silver print',\n",
    " 'technique_letterpress printing',\n",
    " 'technique_drypoint',\n",
    " 'technique_salted paper print',\n",
    " 'technique_slide',\n",
    " 'technique_painting',\n",
    " 'technique_steel engraving']\n",
    "\n",
    "def technique_encoder(input_df:pd.DataFrame, left_df=technique, tg_col=tg_techcol):\n",
    "    # 技法数\n",
    "    out_df = input_df[['object_id']].copy()\n",
    "    vc = technique['object_id'].value_counts()\n",
    "    out_df['tech_cnt'] = input_df['object_id'].map(vc).fillna(-1)\n",
    "    \n",
    "    # 主要な技法の使用\n",
    "    _ohe = pd.concat([left_df['object_id'], pd.get_dummies(left_df['name'])], axis=1)\n",
    "    _ohe = _ohe.groupby('object_id').max().add_prefix(f'technique_')\n",
    "    _ohe = _ohe[tg_techcol]\n",
    "    out_df = out_df.merge(_ohe, left_on='object_id', right_index=True, how='left').fillna(0)\n",
    "\n",
    "    return out_df.drop('object_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, technique_encoder(train)], axis=1)\n",
    "test = pd.concat([test, technique_encoder(test)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: object_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 関数化_最終\n",
    "obj_coll_tgcol = [\n",
    " 'obj_coll_prints',\n",
    " 'obj_coll_paintings',\n",
    " 'obj_coll_Navy Model Room',\n",
    " 'obj_coll_paper',\n",
    " 'obj_coll_drawings',\n",
    " 'obj_coll_dollhouse',\n",
    " 'obj_coll_lace',\n",
    " 'obj_coll_musical instruments'\n",
    "]\n",
    "\n",
    "\n",
    "def obj_coll_encoder(input_df:pd.DataFrame, left_df=object_collection, tg_col=obj_coll_tgcol):\n",
    "    # 技法数\n",
    "    out_df = input_df[['object_id']].copy()\n",
    "    vc = left_df['object_id'].value_counts()\n",
    "    out_df['obj_coll_cnt'] = input_df['object_id'].map(vc).fillna(-1)\n",
    "    \n",
    "    # 主要な技法の使用\n",
    "    _ohe = pd.concat([left_df['object_id'], pd.get_dummies(left_df['name'])], axis=1)\n",
    "    _ohe = _ohe.groupby('object_id').max().add_prefix(f'obj_coll_')\n",
    "    \n",
    "    if len(tg_col) != 0:\n",
    "        _ohe = _ohe[tg_col]\n",
    "\n",
    "    out_df = out_df.merge(_ohe, left_on='object_id', right_index=True, how='left').fillna(0)\n",
    "\n",
    "    return out_df.drop('object_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, obj_coll_encoder(train)], axis=1)\n",
    "test = pd.concat([test, obj_coll_encoder(test)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 関数化\n",
    "tg_material_col = [\n",
    " 'material_paper',\n",
    " 'material_photographic paper',\n",
    " 'material_oil paint (paint)',\n",
    " 'material_cardboard',\n",
    " 'material_canvas',\n",
    " 'material_panel',\n",
    " 'material_glass',\n",
    " 'material_copper (metal)',\n",
    " 'material_baryta paper']\n",
    "\n",
    "def material_encoder(input_df:pd.DataFrame, left_df=material, tg_col=tg_material_col, prefix='material_'):\n",
    "    # 技法数\n",
    "    out_df = input_df[['object_id']].copy()\n",
    "    vc = left_df['object_id'].value_counts()\n",
    "    out_df[f'{prefix}cnt'] = input_df['object_id'].map(vc).fillna(-1)\n",
    "    \n",
    "    # 主要な技法の使用\n",
    "    _ohe = pd.concat([left_df['object_id'], pd.get_dummies(left_df['name'])], axis=1)\n",
    "    _ohe = _ohe.groupby('object_id').max().add_prefix(f'{prefix}')\n",
    "    \n",
    "    if len(tg_col) != 0:\n",
    "        _ohe = _ohe[tg_col]\n",
    "\n",
    "    out_df = out_df.merge(_ohe, left_on='object_id', right_index=True, how='left').fillna(0)\n",
    "\n",
    "    return out_df.drop('object_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, material_encoder(train)], axis=1)\n",
    "test = pd.concat([test, material_encoder(test)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: production_place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 関数化\n",
    "tg_production_place_col = [\n",
    " 'production_place_Amsterdam',\n",
    " 'production_place_Netherlands',\n",
    " 'production_place_Northern Netherlands',\n",
    " 'production_place_Antwerp',\n",
    " 'production_place_Paris',\n",
    " 'production_place_France',\n",
    " 'production_place_The Hague',\n",
    " 'production_place_unknown',\n",
    " 'production_place_Haarlem',\n",
    " 'production_place_Cologne',\n",
    " 'production_place_Rotterdam',\n",
    " 'production_place_Utrecht',\n",
    " 'production_place_Germany',\n",
    " 'production_place_Suriname',\n",
    " 'production_place_? Netherlands',\n",
    " 'production_place_Venice',\n",
    " 'production_place_London',\n",
    " 'production_place_Southern Netherlands']\n",
    "\n",
    "def production_place_encoder(input_df:pd.DataFrame,\n",
    "                             left_df=production_place,\n",
    "                             tg_col=tg_production_place_col,\n",
    "                             prefix='production_place_'):\n",
    "    # カウント\n",
    "    out_df = input_df[['object_id']].copy()\n",
    "    vc = left_df['object_id'].value_counts()\n",
    "    out_df[f'{prefix}cnt'] = input_df['object_id'].map(vc).fillna(-1)\n",
    "    \n",
    "    # ?を含むかどうか\n",
    "    _tmp = left_df.copy()\n",
    "    _tmp['production_place_question'] = left_df['name'].map(lambda x: 1 if x[:1] == '?' else 0)\n",
    "    _question = _tmp.groupby('object_id')['production_place_question'].max()\n",
    "    out_df[f'{prefix}question'] = input_df['object_id'].map(_question).fillna(-1)\n",
    "    \n",
    "    # 主要なフラグ\n",
    "    _ohe = pd.concat([left_df['object_id'], pd.get_dummies(left_df['name'])], axis=1)\n",
    "    _ohe = _ohe.groupby('object_id').max().add_prefix(f'{prefix}')\n",
    "    \n",
    "    if len(tg_col) != 0:\n",
    "        _ohe = _ohe[tg_col]\n",
    "\n",
    "    out_df = out_df.merge(_ohe, left_on='object_id', right_index=True, how='left').fillna(0)\n",
    "\n",
    "    return out_df.drop('object_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, production_place_encoder(train)], axis=1)\n",
    "test = pd.concat([test, production_place_encoder(test)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: Text_length\n",
    "- テキスト系カラムの長さを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_length_transformer(input_df:pd.DataFrame,\n",
    "                            columns:list,\n",
    "                            prefix='text_length_'):\n",
    "    out_df = pd.DataFrame()\n",
    "    \n",
    "    for col_ in columns:\n",
    "        out_df[f'{prefix}{col_}'] = input_df[col_].fillna('').map(lambda x: len(x) if x!='' else np.nan)\n",
    "\n",
    "    print(out_df.columns.tolist())\n",
    "\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgcol_length = ['title',\n",
    "                'description',\n",
    "                'more_title',\n",
    "                'long_title',\n",
    "                'principal_maker',\n",
    "                'principal_or_first_maker',\n",
    "                'acquisition_credit_line',\n",
    "                ]\n",
    "\n",
    "train = pd.concat([train, text_length_transformer(train, tgcol_length)], axis=1)\n",
    "test = pd.concat([test, text_length_transformer(test, tgcol_length)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess: historical_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_historical_person_col = [\n",
    " 'historical_person_WillemIIIprinsvanOranjeenkoningvanEngelandSchotlandenIerland',\n",
    " 'historical_person_LodewijkXIVkoningvanFrankrijk',\n",
    " 'historical_person_VerenigdeOostindischeCompagnie',\n",
    " 'historical_person_WillemIprinsvanOranje',\n",
    " 'historical_person_lvarezdeToledoFernando3ehertogvanAlva',\n",
    " 'historical_person_WilhelminakoninginderNederlanden',\n",
    " 'historical_person_WittJohande',\n",
    " 'historical_person_WittCornelisde',\n",
    " 'historical_person_Wehrmacht',\n",
    " 'historical_person_DjatirotoSuikeronderneming']\n",
    "\n",
    "def clean_words(list_:list, stopwords=['']):\n",
    "    list_drop_stopwords = [i for i in list_ if i not in stopwords]\n",
    "    return [re.sub('[^A-Za-z0-9_]+', '', x) for x in list_drop_stopwords]\n",
    "\n",
    "\n",
    "def historical_person_encoder(input_df:pd.DataFrame,\n",
    "                             left_df=historical_person,\n",
    "                             tg_col=tg_historical_person_col,\n",
    "                             prefix='historical_person_'):\n",
    "    _tmp_df = left_df.copy()\n",
    "    _tmp_df['name'] = _tmp_df['name'].map(lambda x: ''.join(clean_words(x)))\n",
    "    \n",
    "    # カウント\n",
    "    out_df = input_df[['object_id']].copy()\n",
    "    vc = _tmp_df['object_id'].value_counts()\n",
    "    out_df[f'{prefix}cnt'] = input_df['object_id'].map(vc).fillna(-1)\n",
    "    \n",
    "    # 主要なフラグ\n",
    "    _ohe = pd.concat([_tmp_df['object_id'], pd.get_dummies(_tmp_df['name'])], axis=1)\n",
    "    _ohe = _ohe.groupby('object_id').max().add_prefix(f'{prefix}')\n",
    "    \n",
    "    if len(tg_col) != 0:\n",
    "        _ohe = _ohe[tg_col]\n",
    "\n",
    "    out_df = out_df.merge(_ohe, left_on='object_id', right_index=True, how='left').fillna(0)\n",
    "\n",
    "    return out_df.drop('object_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, historical_person_encoder(train)], axis=1)\n",
    "test = pd.concat([test, historical_person_encoder(test)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature: bool columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# そのまま渡すパターン\n",
    "def boolean_converter(data, bool_columns):\n",
    "    return data[bool_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特定のラベルをフラグとするパターン\n",
    "\n",
    "# カラム・文字列を辞書としてリストに格納\n",
    "\n",
    "def boolean_word_converter(data, boolcol_labels):\n",
    "    out_df = pd.DataFrame()\n",
    "    for dic_ in boolcol_labels:\n",
    "        out_df[dic_['col']] = data[dic_['col']].apply(lambda x: 1 if x == dic_['label'] else 0)\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "bool_columns = [*tg_techcol,\n",
    "                *obj_coll_tgcol,\n",
    "                *tg_material_col,\n",
    "                *tg_production_place_col]\n",
    "\n",
    "boolcol_labels = [\n",
    "           {'col':'principal_maker_nationality',\n",
    "            'label':'dutch'\n",
    "           }\n",
    "]\n",
    "\n",
    "assert len(train) == len(boolean_converter(train, bool_columns))\n",
    "assert len(train) == len(boolean_word_converter(train, boolcol_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature: null_or_exists"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# NULLの多い特徴量を「存在するかどうか」に変換\n",
    "many_nulls = ['square_feet', 'host_response_time', 'host_response_rate']\n",
    "\n",
    "def exists_features(input_df:pd.DataFrame, columns:list):\n",
    "    out_df = pd.DataFrame()\n",
    "    for col_ in columns:\n",
    "        out_df[col_] = input_df[col_].notnull()\n",
    "        \n",
    "    return out_df.add_prefix(f'exists_')    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_feat = pd.concat([train_feat, exists_features(train, many_nulls)], axis=1)\n",
    "test_feat = pd.concat([test_feat, exists_features(test, many_nulls)], axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(train_feat.shape)\n",
    "print(test_feat.shape)\n",
    "print(train_feat.columns)\n",
    "print(test_feat.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature: Category Encoding\n",
    "- LE, CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding系特徴量。ベースとなる継承元のクラス\n",
    "class BaseBlock(object):\n",
    "    def fit(self, input_df, y=None):\n",
    "        return self.transform(input_df)\n",
    "    def transform(self, input_df):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelEncoding\n",
    "class LabelBlock(BaseBlock):\n",
    "    def __init__(self, column: str, whole_df: pd.DataFrame):\n",
    "        self.column = column\n",
    "        self.le = LabelEncoder()\n",
    "        self.whole_df = whole_df\n",
    "\n",
    "    def fit(self, input_df, y=None):\n",
    "        self.le.fit(self.whole_df[self.column].fillna('nan'))\n",
    "        return self.transform(input_df)\n",
    "    \n",
    "    def transform(self, input_df):\n",
    "        c = self.column\n",
    "        out_df = pd.DataFrame()\n",
    "        out_df[c] = self.le.transform(input_df[self.column].fillna('nan')).astype('int')\n",
    "        return out_df.add_prefix(f'LE_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Encoding\n",
    "class CountEncodingBlock(BaseBlock):\n",
    "    def __init__(self, column, whole_df: pd.DataFrame):\n",
    "        self.column = column\n",
    "        self.whole_df = whole_df\n",
    "    \n",
    "    def transform(self, input_df):\n",
    "        output_df = pd.DataFrame()\n",
    "        c = self.column\n",
    "        \n",
    "        vc = self.whole_df[c].value_counts()\n",
    "        output_df[c] = input_df[c].map(vc)\n",
    "        return output_df.add_prefix('CE_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "check_categ = 'principal_maker'\n",
    "assert len(train) == len(LabelBlock(check_categ, whole_df=train).fit(train))\n",
    "assert len(train) == len(CountEncodingBlock(check_categ, whole_df=train).fit(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature: numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_converter(data, numerics):\n",
    "    return data[numerics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "numerics = ['dating_sorting_date']\n",
    "assert len(train) == len(numeric_converter(train, numerics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature: tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考: https://www.guruguru.science/competitions/16/discussions/556029f7-484d-40d4-ad6a-9d86337487e2/\n",
    "# 一旦、まんま同じコードで実装。後で深堀りして理解する。\n",
    "\n",
    "def text_normalization(text):\n",
    "\n",
    "    # 英語とオランダ語を stopword として指定\n",
    "    custom_stopwords = nltk.corpus.stopwords.words('dutch') + nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    x = hero.clean(text, pipeline=[\n",
    "        hero.preprocessing.fillna,\n",
    "        hero.preprocessing.lowercase,\n",
    "        hero.preprocessing.remove_digits,\n",
    "        hero.preprocessing.remove_punctuation,\n",
    "        hero.preprocessing.remove_diacritics,\n",
    "        lambda x: hero.preprocessing.remove_stopwords(x, stopwords=custom_stopwords)\n",
    "    ])\n",
    "\n",
    "    return x\n",
    "\n",
    "class TfidfBlock(BaseBlock):\n",
    "    \"\"\"tfidf x SVD による圧縮を行なう block\"\"\"\n",
    "    def __init__(self, column: str):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            column: str\n",
    "                変換対象のカラム名\n",
    "        \"\"\"\n",
    "        self.column = column\n",
    "\n",
    "    def preprocess(self, input_df):\n",
    "        x = text_normalization(input_df[self.column])\n",
    "        return x\n",
    "\n",
    "    def get_master(self, input_df):\n",
    "        \"\"\"tdidfを計算するための全体集合を返す. \n",
    "        デフォルトでは fit でわたされた dataframe を使うが, もっと別のデータを使うのも考えられる.\"\"\"\n",
    "        return input_df\n",
    "\n",
    "    def fit(self, input_df, y=None, n_components=50):\n",
    "        master_df = self.get_master(input_df)\n",
    "        text = self.preprocess(input_df)\n",
    "        self.pileline_ = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(max_features=10000)),\n",
    "            ('svd', TruncatedSVD(n_components=n_components, random_state=SEED)),\n",
    "        ])\n",
    "\n",
    "        self.pileline_.fit(text)\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        text = self.preprocess(input_df)\n",
    "        z = self.pileline_.transform(text)\n",
    "\n",
    "        out_df = pd.DataFrame(z)\n",
    "        return out_df.add_prefix(f'{self.column}_tfidf_')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "block = TfidfBlock('title')\n",
    "block.fit(train_base)\n",
    "assert block.transform(train_base).equals(block.transform(train_base))\n",
    "assert block.transform(train_base).equals(block.transform(train_base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "block = TfidfBlock('title')\n",
    "block.fit(train_base)\n",
    "\n",
    "assert block.transform(train_base).equals(block.transform(train_base))\n",
    "assert block.transform(train_base).equals(block.transform(train_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature: Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby系\n",
    "# 参考: https://github.com/Ynakatsuka/kaggle_utils\n",
    "def preprocess(data):\n",
    "    return data\n",
    "        \n",
    "class GroupbyBlock(BaseBlock):\n",
    "    \"\"\"Groupby系特徴量\"\"\"\n",
    "    def __init__(self, param_dict, whole_df: pd.DataFrame):\n",
    "        self.param_dict = param_dict\n",
    "        self.whole_df = preprocess(whole_df)\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        out_df = pd.DataFrame()\n",
    "        for param_dict in self.param_dict:\n",
    "            key, var, agg, on = self._get_params(param_dict)\n",
    "            all_features = list(set(key + var))\n",
    "            new_features = self._get_feature_names(key, var, agg)\n",
    "            features = self.whole_df[all_features].groupby(key)[\n",
    "                var].agg(agg).reset_index()\n",
    "            features.columns = key + new_features\n",
    "            print(key)\n",
    "            feat_df = pd.merge(input_df[key], features, on=on, how='left').drop(columns=key)\n",
    "            out_df = pd.concat([out_df, feat_df], axis=1).fillna(0)\n",
    "        return out_df   \n",
    "\n",
    "    def _get_params(self, p_dict):\n",
    "        key = p_dict['key']\n",
    "        if 'var' in p_dict.keys():\n",
    "            var = p_dict['var']\n",
    "        else:\n",
    "            var = self.var\n",
    "        if 'agg' in p_dict.keys():\n",
    "            agg = p_dict['agg']\n",
    "        else:\n",
    "            agg = self.agg\n",
    "        if 'on' in p_dict.keys():\n",
    "            on = p_dict['on']\n",
    "        else:\n",
    "            on = key\n",
    "        return key, var, agg, on\n",
    "\n",
    "    def _get_feature_names(self, key, var, agg):\n",
    "        _agg = []\n",
    "        for a in agg:\n",
    "            if not isinstance(a, str):\n",
    "                _agg.append(a.__name__)\n",
    "            else:\n",
    "                _agg.append(a)\n",
    "        return ['_'.join([a, v, 'groupby'] + key) for v in var for a in _agg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "param_dict_grp = [\n",
    "            # acquisition_date\n",
    "            {# フラグ系\n",
    "                'key': ['acquisition_date'], \n",
    "                'var': ['dating_presenting_date'\n",
    "                       ], \n",
    "                'agg': ['nunique']\n",
    "            }]\n",
    "assert len(train) == len(GroupbyBlock(param_dict_grp, whole_df=train).fit(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴作成_パラメタ定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boolean_そのまま渡すパターン\n",
    "bool_columns = [*tg_techcol,\n",
    "                *obj_coll_tgcol,\n",
    "                *tg_material_col,\n",
    "                *tg_production_place_col,\n",
    "                *tg_historical_person_col]\n",
    "\n",
    "# boolean_特定の文字列をフラグとするパターン\n",
    "boolcol_labels = [\n",
    "           {'col':'principal_maker_nationality',\n",
    "            'label':'dutch'\n",
    "           }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LE, CE\n",
    "le_categories = ['principal_maker',\n",
    "              'principal_or_first_maker',\n",
    "              'copyright_holder', \n",
    "              'acquisition_method', \n",
    "              #'acquisition_date', \n",
    "              #'dating_presenting_date',\n",
    "              'principal_maker_place_of_birth',\n",
    "              'principal_maker_date_of_birth',\n",
    "              'principal_maker_date_of_death',\n",
    "              'principal_maker_place_of_death',\n",
    "             'title_lang_ft',\n",
    "             'description_lang_ft'\n",
    "             #'long_title_lang_ft'\n",
    "             #'more_title_lang_ft'\n",
    "                ]\n",
    "\n",
    "ce_categories = ['art_series_id',\n",
    "                 'title',\n",
    "                 'description',\n",
    "                 'long_title',\n",
    "                 'principal_maker',\n",
    "                 'principal_or_first_maker',\n",
    "                 'sub_title',\n",
    "                 'copyright_holder',\n",
    "                 'more_title',\n",
    "                 'acquisition_date',\n",
    "                 'acquisition_credit_line',\n",
    "                 'dating_presenting_date',\n",
    "                 'title_lang_ft',\n",
    "                 'description_lang_ft',\n",
    "                 #'long_title_lang_ft'\n",
    "                 #'more_title_lang_ft'\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_palette\n",
    "feat_palette = [\n",
    " 'max_ratio',\n",
    " 'max_palette_r',\n",
    " 'max_palette_g',\n",
    " 'max_palette_b',\n",
    " #'ratio_x',\n",
    " 'mean_palette_r',\n",
    " 'mean_palette_g',\n",
    " 'mean_palette_b',\n",
    " #'ratio_y',\n",
    " 'var_palette_r',\n",
    " 'var_palette_g',\n",
    " 'var_palette_b']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerics\n",
    "numerics = ['dating_sorting_date',\n",
    "            'dating_period', \n",
    "            'dating_year_early', \n",
    "            'dating_year_late',\n",
    "            'dating_presenting_year_num',\n",
    "            'tech_cnt',\n",
    "            'obj_coll_cnt',\n",
    "            'material_cnt',\n",
    "            'production_place_question',\n",
    "            'production_place_cnt',\n",
    "            'acquisition_date_year',\n",
    "            'acquisition_date_ym',\n",
    "            'acquisition_date_month',\n",
    "            'diff_acquisition_date_year__dating_year_late',\n",
    "            'size_h', 'size_w', 'size_t', 'size_d',\n",
    "            'size_square',\n",
    "            'text_length_title',\n",
    "            'text_length_description',\n",
    "            'text_length_more_title',\n",
    "            'text_length_long_title',\n",
    "            'text_length_principal_maker',\n",
    "            'text_length_principal_or_first_maker',\n",
    "            'text_length_acquisition_credit_line'\n",
    "            #'size_aspect_ratio',\n",
    "            #*feat_palette,\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキスト特徴\n",
    "text_cols = ['title',\n",
    "            'description',\n",
    "            'more_title',\n",
    "            'long_title',\n",
    "            'acquisition_credit_line'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby(長いので、本当は外部で定義するようにしたい)\n",
    "param_dict_grp = [\n",
    "            # acquisition_date\n",
    "            {# フラグ系\n",
    "                'key': ['acquisition_date'], \n",
    "                'var': [*tg_techcol,\n",
    "                        *obj_coll_tgcol,\n",
    "                        *tg_material_col,\n",
    "                        *tg_production_place_col\n",
    "                       ], \n",
    "                'agg': ['sum', 'mean']\n",
    "            },\n",
    "            {# 数値系\n",
    "                'key': ['acquisition_date'], \n",
    "                'var': [*numerics\n",
    "                       ], \n",
    "                'agg': ['min', 'max', 'mean', 'median', 'sum', 'std']\n",
    "            },\n",
    "            {# カテゴリ系\n",
    "                'key': ['acquisition_date'], \n",
    "                'var': [i for i in set([*le_categories, *ce_categories]) if i != 'acquisition_date'],\n",
    "                'agg': ['nunique']\n",
    "            },\n",
    "            # dating_presenting_date\n",
    "            {# フラグ系\n",
    "                'key': ['dating_presenting_date'], \n",
    "                'var': [*tg_techcol,\n",
    "                        *obj_coll_tgcol,\n",
    "                        *tg_material_col,\n",
    "                        *tg_production_place_col\n",
    "                       ], \n",
    "                'agg': ['sum', 'mean']\n",
    "            },\n",
    "            {# 数値系\n",
    "                'key': ['dating_presenting_date'], \n",
    "                'var': [*numerics\n",
    "                       ], \n",
    "                'agg': ['min', 'max', 'mean', 'median', 'sum', 'std']\n",
    "            },\n",
    "            {# カテゴリ系\n",
    "                'key': ['dating_presenting_date'], \n",
    "                'var': [i for i in set([*le_categories, *ce_categories]) if i != 'dating_presenting_date'],\n",
    "                'agg': ['nunique']\n",
    "            },\n",
    "            # principal_or_first_maker\n",
    "            {# フラグ系\n",
    "                'key': ['principal_or_first_maker'], \n",
    "                'var': [*tg_techcol,\n",
    "                        *obj_coll_tgcol,\n",
    "                        *tg_material_col,\n",
    "                        *tg_production_place_col\n",
    "                       ], \n",
    "                'agg': ['sum', 'mean']\n",
    "            },\n",
    "            {# 数値系\n",
    "                'key': ['principal_or_first_maker'], \n",
    "                'var': [*numerics\n",
    "                       ], \n",
    "                'agg': ['min', 'max', 'mean', 'median', 'sum', 'std']\n",
    "            },\n",
    "            {# カテゴリ系\n",
    "                'key': ['principal_or_first_maker'], \n",
    "                'var': [i for i in set([*le_categories, *ce_categories]) if i != 'principal_or_first_maker'],\n",
    "                'agg': ['nunique']\n",
    "            },\n",
    "            # principal_maker_place_of_birth\n",
    "            {# フラグ系\n",
    "                'key': ['principal_maker_place_of_birth'], \n",
    "                'var': [*tg_techcol,\n",
    "                        *obj_coll_tgcol,\n",
    "                        *tg_material_col,\n",
    "                        *tg_production_place_col\n",
    "                       ], \n",
    "                'agg': ['sum', 'mean']\n",
    "            },\n",
    "            {# 数値系\n",
    "                'key': ['principal_maker_place_of_birth'], \n",
    "                'var': [*numerics\n",
    "                       ], \n",
    "                'agg': ['min', 'max', 'mean', 'median', 'sum', 'std']\n",
    "            },\n",
    "            {# カテゴリ系\n",
    "                'key': ['principal_maker_place_of_birth'], \n",
    "                'var': [i for i in set([*le_categories, *ce_categories]) if i != 'principal_maker_place_of_birth'],\n",
    "                'agg': ['nunique']\n",
    "            },\n",
    "            # principal_maker_date_of_birth\n",
    "            {# フラグ系\n",
    "                'key': ['principal_maker_date_of_birth'], \n",
    "                'var': [*tg_techcol,\n",
    "                        *obj_coll_tgcol,\n",
    "                        *tg_material_col,\n",
    "                        *tg_production_place_col\n",
    "                       ], \n",
    "                'agg': ['sum', 'mean']\n",
    "            },\n",
    "            {# 数値系\n",
    "                'key': ['principal_maker_date_of_birth'], \n",
    "                'var': [*numerics\n",
    "                       ], \n",
    "                'agg': ['min', 'max', 'mean', 'median', 'sum', 'std']\n",
    "            },\n",
    "            {# カテゴリ系\n",
    "                'key': ['principal_maker_date_of_birth'], \n",
    "                'var': [i for i in set([*le_categories, *ce_categories]) if i != 'principal_maker_date_of_birth'],\n",
    "                'agg': ['nunique']\n",
    "            },\n",
    "            # principal_maker\n",
    "            {# フラグ系\n",
    "                'key': ['principal_maker'], \n",
    "                'var': [*tg_techcol,\n",
    "                        *obj_coll_tgcol,\n",
    "                        *tg_material_col,\n",
    "                        *tg_production_place_col\n",
    "                       ], \n",
    "                'agg': ['sum', 'mean']\n",
    "            },\n",
    "            {# 数値系\n",
    "                'key': ['principal_maker'], \n",
    "                'var': [*numerics\n",
    "                       ], \n",
    "                'agg': ['min', 'max', 'mean', 'median', 'sum', 'std']\n",
    "            },\n",
    "            {# カテゴリ系\n",
    "                'key': ['principal_maker'], \n",
    "                'var': [i for i in set([*le_categories, *ce_categories]) if i != 'principal_maker'],\n",
    "                'agg': ['nunique']\n",
    "            }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カラー特徴(paletteベース)\n",
    "feat_palette_cv = ['ratio',\n",
    " 'max_lab_L',\n",
    " 'max_lab_a',\n",
    " 'max_lab_b',\n",
    " 'mean_lab_L',\n",
    " 'mean_lab_a',\n",
    " 'mean_lab_b',\n",
    " 'var_lab_L',\n",
    " 'var_lab_a',\n",
    " 'var_lab_b',\n",
    " 'std_lab_L',\n",
    " 'std_lab_a',\n",
    " 'std_lab_b'\n",
    "                  ]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# カラー特徴(paletteベース)\n",
    "feat_palette_cv = ['ratio',\n",
    " 'max_lab_L',\n",
    " 'max_lab_a',\n",
    " 'max_lab_b',\n",
    " 'max_Luv_L',\n",
    " 'max_Luv_u',\n",
    " 'max_Luv_v',\n",
    " 'max_XYZ_X',\n",
    " 'max_XYZ_Y',\n",
    " 'max_XYZ_Z',\n",
    " 'max_grayscale',\n",
    " 'mean_lab_L',\n",
    " 'mean_lab_a',\n",
    " 'mean_lab_b',\n",
    " 'mean_Luv_L',\n",
    " 'mean_Luv_u',\n",
    " 'mean_Luv_v',\n",
    " 'mean_XYZ_X',\n",
    " 'mean_XYZ_Y',\n",
    " 'mean_XYZ_Z',\n",
    " 'mean_grayscale',\n",
    " 'var_lab_L',\n",
    " 'var_lab_a',\n",
    " 'var_lab_b',\n",
    " 'var_Luv_L',\n",
    " 'var_Luv_u',\n",
    " 'var_Luv_v',\n",
    " 'var_XYZ_X',\n",
    " 'var_XYZ_Y',\n",
    " 'var_XYZ_Z',\n",
    " 'var_grayscale',\n",
    " 'std_lab_L',\n",
    " 'std_lab_a',\n",
    " 'std_lab_b',\n",
    " 'std_Luv_L',\n",
    " 'std_Luv_u',\n",
    " 'std_Luv_v',\n",
    " 'std_XYZ_X',\n",
    " 'std_XYZ_Y',\n",
    " 'std_XYZ_Z'\n",
    " 'std_grayscale'\n",
    "                  ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴作成_実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat用df初期化\n",
    "train_feat = pd.DataFrame()\n",
    "test_feat = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boolean\n",
    "train_feat = pd.concat([train_feat, boolean_converter(train, bool_columns)], axis=1)\n",
    "test_feat  = pd.concat([test_feat, boolean_converter(test, bool_columns)], axis=1)\n",
    "train_feat = pd.concat([train_feat, boolean_word_converter(train, boolcol_labels)], axis=1)\n",
    "test_feat  = pd.concat([test_feat, boolean_word_converter(test, boolcol_labels)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行_LE\n",
    "le_blocks = [*[LabelBlock(c, whole_df=pd.concat([train, test], axis=0)) for c in le_categories]]\n",
    "le_dic = {}  # LEのオブジェクトを辞書に格納しておく。SHAPのラベル用。\n",
    "\n",
    "for i, block in enumerate(le_blocks):\n",
    "    le_feat = block.fit(train)\n",
    "    cur_categ = le_categories[i]\n",
    "    le_dic[cur_categ] = block.le\n",
    "    train_feat = pd.concat([train_feat, le_feat], axis=1)\n",
    "    test_feat = pd.concat([test_feat, block.fit(test)], axis=1)\n",
    "    \n",
    "# 実行_CE\n",
    "ce_blocks = [*[CountEncodingBlock(c, whole_df=pd.concat([train, test], axis=0)) for c in ce_categories]]\n",
    "for i, block in enumerate(ce_blocks):\n",
    "    train_feat = pd.concat([train_feat, block.fit(train)], axis=1)\n",
    "    test_feat = pd.concat([test_feat, block.fit(test)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行_numerics\n",
    "train_feat = pd.concat([train_feat, numeric_converter(train, numerics)], axis=1)\n",
    "test_feat  = pd.concat([test_feat, numeric_converter(test, numerics)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行_tfidf\n",
    "for col_ in text_cols:\n",
    "    block = TfidfBlock(col_)\n",
    "    block.fit(pd.concat([train, test], axis=0))\n",
    "\n",
    "    train_feat = pd.concat([train_feat, block.transform(train)], axis=1)\n",
    "    test_feat  = pd.concat([test_feat, block.transform(test)], axis=1)\n",
    "    print(col_)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 実行_tfidf_technique\n",
    "block = TfidfBlock('tech_mate_objc')\n",
    "block.fit(pd.concat([train, test], axis=0), n_components=3)\n",
    "\n",
    "train_feat = pd.concat([train_feat, block.transform(train)], axis=1)\n",
    "test_feat  = pd.concat([test_feat, block.transform(test)], axis=1)\n",
    "print(col_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupbyしたくないnumericカラムはここで渡す\n",
    "# 実行_W2V\n",
    "train_feat = pd.concat([train_feat, boolean_converter(train, w2v_cols)], axis=1)\n",
    "test_feat  = pd.concat([test_feat, boolean_converter(test, w2v_cols)], axis=1)\n",
    "train_feat = pd.concat([train_feat, boolean_converter(train, w2v_cols_text)], axis=1)\n",
    "test_feat  = pd.concat([test_feat, boolean_converter(test, w2v_cols_text)], axis=1)\n",
    "\n",
    "# 実行_palette\n",
    "train_feat = pd.concat([train_feat, boolean_converter(train, feat_palette_cv)], axis=1)\n",
    "test_feat  = pd.concat([test_feat, boolean_converter(test, feat_palette_cv)], axis=1)\n",
    "train_feat = pd.concat([train_feat, boolean_converter(train, feat_palette_hsvyiq)], axis=1)\n",
    "test_feat  = pd.concat([test_feat, boolean_converter(test, feat_palette_hsvyiq)], axis=1)\n",
    "\n",
    "# 実行_color\n",
    "train_feat = pd.concat([train_feat, boolean_converter(train, color_columns)], axis=1)\n",
    "test_feat  = pd.concat([test_feat, boolean_converter(test, color_columns)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行_Groupby\n",
    "train_grp = GroupbyBlock(param_dict_grp, whole_df=pd.concat([train, test])).fit(train)\n",
    "test_grp = GroupbyBlock(param_dict_grp, whole_df=pd.concat([train, test])).fit(test)\n",
    "train_feat = pd.concat([train_feat, train_grp], axis=1)\n",
    "test_feat = pd.concat([test_feat, test_grp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴出力用カラム指定＆作成\n",
    "FEAT_SELECT_DIR_IN = FEAT_DIR/f'feat_select/{feat_input}'\n",
    "FEAT_SELECT_DIR_OUT = FEAT_DIR/f'feat_select/{feat_output}'\n",
    "FEAT_SELECT_DIR_OUT.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature_selection: stdが0のカラム、他と完全相関しているカラムを除外\n",
    "- ノイズとなる特徴を除外する処理。\n",
    "- 特徴を(手動で)選ぶ上でのノイズを減らしたいための処理で、実際のところ精度は全く変わらなかった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_feat = pd.concat([train_feat, test_feat], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_FEAT_SELECTION == True:\n",
    "    # stdが0のカラムを抽出、full_featから除外\n",
    "    cols_std0 = full_feat.std()[full_feat.std() == 0].index.tolist()\n",
    "\n",
    "    with open(FEAT_SELECT_DIR_OUT/f'cols_std0.pkl', 'wb') as f:\n",
    "        pickle.dump(cols_std0 , f)\n",
    "\n",
    "    with open(FEAT_SELECT_DIR_OUT/f'cols_std0.pkl', 'wb') as f:\n",
    "        pickle.dump(cols_std0 , f)\n",
    "\n",
    "else:    \n",
    "    with open(FEAT_SELECT_DIR_IN/f'cols_std0.pkl', 'rb') as f:\n",
    "        cols_std0 = pickle.load(f)\n",
    "\n",
    "    with open(FEAT_SELECT_DIR_IN/f'cols_std0.pkl', 'rb') as f:\n",
    "        cols_std0 = pickle.load(f)\n",
    "\n",
    "full_feat = full_feat.drop(cols_std0, axis=1)\n",
    "print(cols_std0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_FEAT_SELECTION == True:\n",
    "    # 相関係数を求め、完全相関のカラムを判別\n",
    "    full_corr = full_feat.corr()\n",
    "\n",
    "    def return_perfect_correlation(sr_:pd.Series):\n",
    "        perf_cols = sr_[sr_ == 1].index.drop(sr_.name).tolist()\n",
    "        if len(perf_cols) == 0:\n",
    "            return 'NoColumns'\n",
    "        else:\n",
    "            return perf_cols\n",
    "\n",
    "    full_corr_ = full_corr.apply(return_perfect_correlation, axis='columns')\n",
    "    full_corr_perf = full_corr_[full_corr_ != 'NoColumns']\n",
    "    \n",
    "    # 完全相関のカラムを格納\n",
    "    cols_perfcorr = []\n",
    "\n",
    "    for idx_, list_ in zip(full_corr_perf.index, full_corr_perf.values):\n",
    "        #print(idx_, list_)\n",
    "        if idx_ in full_feat.columns:\n",
    "            for tgcol_ in list_:\n",
    "                full_feat = full_feat.drop(tgcol_, axis='columns')\n",
    "                cols_perfcorr.append(tgcol_)\n",
    "                print(tgcol_)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "    with open(FEAT_SELECT_DIR_OUT/f'cols_perfcorr.pkl', 'wb') as f:\n",
    "        pickle.dump(cols_perfcorr , f)\n",
    "\n",
    "    with open(FEAT_SELECT_DIR_OUT/f'cols_perfcorr.pkl', 'wb') as f:\n",
    "        pickle.dump(cols_perfcorr , f)\n",
    "\n",
    "else:    \n",
    "    with open(FEAT_SELECT_DIR_IN/f'cols_perfcorr.pkl', 'rb') as f:\n",
    "        cols_perfcorr = pickle.load(f)\n",
    "\n",
    "    with open(FEAT_SELECT_DIR_IN/f'cols_perfcorr.pkl', 'rb') as f:\n",
    "        cols_perfcorr = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上記で求めた不要カラムを除外\n",
    "print(train_feat.shape)\n",
    "print(test_feat.shape)\n",
    "\n",
    "train_feat = train_feat.drop(cols_perfcorr + cols_std0, axis=1)\n",
    "test_feat = test_feat.drop(cols_perfcorr + cols_std0, axis=1)\n",
    "\n",
    "print(train_feat.shape)\n",
    "print(test_feat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train & predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル格納用フォルダ\n",
    "TUNED_MODEL_DIR_IN = FEAT_DIR/f'tuned_models/{tuned_model_input}'\n",
    "TUNED_MODEL_DIR_OUT = FEAT_DIR/f'tuned_models/{tuned_model_output}'\n",
    "TUNED_MODEL_DIR_OUT.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# チューニングしない場合、過去実行済みのモデルを取得\n",
    "if RUN_OPTUNA == False:\n",
    "    with open(TUNED_MODEL_DIR_IN/f'models.pkl', 'rb') as f:\n",
    "        models = pickle.load(f)\n",
    "\n",
    "    param_tuned = []\n",
    "    for i in models:\n",
    "        param_tuned.append(i.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "def kfold_cv(X, y, n_splits=5, random_state=0):\n",
    "    folds = KFold(n_splits=n_splits, random_state=0, shuffle=True)\n",
    "    return list(folds.split(X, y))\n",
    "\n",
    "def stratified_kfold_cv(X, y, n_splits=5, random_state=0):\n",
    "    folds = StratifiedKFold(n_splits=n_splits, random_state=0, shuffle=True)\n",
    "    return list(folds.split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encoding_for_cv(tr_x, tr_y, va_x, te_x, cat_cols, id_cols, seed=SEED):\n",
    "    tr_x_out = pd.merge(tr_x, train[id_cols], left_index=True, right_index=True, how='left')\n",
    "    va_x_out = pd.merge(va_x, train[id_cols], left_index=True, right_index=True, how='left')\n",
    "    te_x_out = pd.merge(te_x, test[id_cols], left_index=True, right_index=True, how='left')\n",
    "\n",
    "    # 変数をループしてtarget encoding\n",
    "    for c in cat_cols:\n",
    "        # 学習データ全体で各カテゴリにおけるtargetの平均を計算\n",
    "        data_tmp = pd.DataFrame({c: tr_x_out[c], 'target': tr_y})\n",
    "        target_mean = data_tmp.groupby(c)['target'].mean()\n",
    "        target_max = data_tmp.groupby(c)['target'].max()\n",
    "\n",
    "        # バリデーションデータ, テストデータのカテゴリを置換\n",
    "        va_x_out.loc[:, f'TGE_Mean_{c}'] = va_x_out[c].map(target_mean)\n",
    "        te_x_out.loc[:, f'TGE_Mean_{c}'] = te_x_out[c].map(target_mean)\n",
    "\n",
    "        # 学習データの変換後の値を格納する配列を準備\n",
    "        tmp = np.repeat(np.nan, tr_x_out.shape[0])\n",
    "        kf_encoding = KFold(n_splits=4, shuffle=True, random_state=seed)\n",
    "        for idx_1, idx_2 in kf_encoding.split(tr_x_out):\n",
    "            # out-of-foldで各カテゴリにおける目的変数の平均を計算\n",
    "            target_mean = data_tmp.iloc[idx_1].groupby(c)['target'].mean()\n",
    "            # 変換後の値を一時配列に格納\n",
    "            tmp[idx_2] = tr_x_out[c].iloc[idx_2].map(target_mean)\n",
    "\n",
    "        tr_x_out.loc[:, f'TGE_Mean_{c}'] = tmp\n",
    "    tr_x_out = tr_x_out.drop(id_cols, axis=1)\n",
    "    va_x_out = va_x_out.drop(id_cols, axis=1)\n",
    "    te_x_out = te_x_out.drop(id_cols, axis=1)\n",
    "\n",
    "    return tr_x_out, va_x_out, te_x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'likes'\n",
    "#cv = kfold_cv(train_feat, train[target], n_splits=3)\n",
    "cv = kfold_cv(train_feat, train[target])\n",
    "#cv = stratified_kfold_cv(train_feat, train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSLE対策に対数変換をかける\n",
    "train_target = np.log1p(train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# チューニングする場合の処理\n",
    "if RUN_OPTUNA == True:\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metrics': 'rmse',\n",
    "        'seed': SEED\n",
    "    }\n",
    "\n",
    "    import optuna.integration.lightgbm as lgb  # lgbをoptunaで再呼び出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "id_cols = ['principal_maker']\n",
    "cat_cols = ['principal_maker']\n",
    "\n",
    "oof_preds = np.zeros(len(train_feat))\n",
    "test_preds = np.zeros(len(test_feat))\n",
    "\n",
    "importances = pd.DataFrame()\n",
    "scores = []\n",
    "models = []\n",
    "\n",
    "for i, (train_index, valid_index) in enumerate(cv):\n",
    "    print(f'\\nFold {i + 1}')\n",
    "    trn_x, trn_y = train_feat.iloc[train_index], train_target.iloc[train_index]\n",
    "    val_x, val_y = train_feat.iloc[valid_index], train_target.iloc[valid_index]\n",
    "    trn_x, val_x, test_feat_tge = target_encoding_for_cv(trn_x, trn_y, val_x, test_feat, cat_cols, id_cols)\n",
    "\n",
    "    dtrain = lgb.Dataset(trn_x, trn_y, categorical_feature = [col_ for col_ in train_feat.columns if col_[:3] == 'LE_'])\n",
    "    dvalid = lgb.Dataset(val_x, val_y, categorical_feature = [col_ for col_ in train_feat.columns if col_[:3] == 'LE_'])\n",
    "\n",
    "    #dtrain = lgb.Dataset(trn_x, trn_y)\n",
    "    #dvalid = lgb.Dataset(val_x, val_y)\n",
    "\n",
    "    if RUN_OPTUNA == False:\n",
    "        params = param_tuned[i] # チューニングしない場合、paramsを事前実行済みのパラメータで上書き(fold毎に) \n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_set=dtrain,\n",
    "        num_boost_round=100000,\n",
    "        valid_sets=[dtrain, dvalid],\n",
    "        valid_names=['training', 'valid'],\n",
    "        early_stopping_rounds=20,\n",
    "        verbose_eval=50\n",
    "    )\n",
    "    \n",
    "    val_preds = model.predict(val_x)\n",
    "    oof_preds[valid_index] = val_preds\n",
    "    test_preds += model.predict(test_feat_tge) / 5\n",
    "    \n",
    "    val_score = model.best_score['valid']['rmse']\n",
    "    scores.append(val_score)\n",
    "    models.append(model)\n",
    "    \n",
    "    imp_df = pd.DataFrame({\n",
    "        'feature': model.feature_name(),\n",
    "        'gain': model.feature_importance(importance_type='gain'),\n",
    "        'fold': i+1\n",
    "    })\n",
    "    \n",
    "    importances = pd.concat([importances, imp_df], axis=0)\n",
    "    \n",
    "mean_score = np.mean(scores)\n",
    "std_score  = np.std(scores)\n",
    "all_score  = np.sqrt(mean_squared_error(train_target, oof_preds))\n",
    "metrics_name = 'RMSE'\n",
    "print(f'Mean {metrics_name}: {mean_score}, std: {std_score}, All {metrics_name}: {all_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_OPTUNA == True:\n",
    "    with open(TUNED_MODEL_DIR_OUT/f'models.pkl', 'wb') as f:\n",
    "        pickle.dump(models , f)\n",
    "        \n",
    "    params_tuned = []\n",
    "    for i in models:\n",
    "        params_tuned.append(i.params)\n",
    "    \n",
    "    with open(TUNED_MODEL_DIR_OUT/f'params_tuned.pkl', 'wb') as f:\n",
    "        pickle.dump(params_tuned , f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 履歴\n",
    "#### nb056\n",
    "Mean RMSE: 0.9795419784472446, std: 0.02287319064231735, All RMSE: 0.9798083364376317\n",
    "\n",
    "#### Tuned\n",
    "Mean RMSE: 0.970350615683453, std: 0.021076563253025746, All RMSE: 0.9705787352541676"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 10))\n",
    "sns.barplot(x='gain', y='feature', data=importances.sort_values('gain', ascending=False)[:100]);\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'feature_importance.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVごとのブレをboxen plotとして表現\n",
    "# 参考: https://www.guruguru.science/competitions/13/discussions/d8f2d66a-aeee-4789-8b3d-d5935c26b1b7/\n",
    "order = importances.groupby('feature')\\\n",
    "    .sum()[['gain']]\\\n",
    "    .sort_values('gain', ascending=False).index[:50]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(max(6, len(order) * .4), 7))\n",
    "sns.boxenplot(data=importances, x='feature', y='gain', order=order, ax=ax, palette='viridis')\n",
    "ax.tick_params(axis='x', rotation=90)\n",
    "ax.grid()\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(OUTPUT_DIR, 'feature_importance_boxen.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAPによる可視化。\n",
    "# 参考その1: https://github.com/slundberg/shap/issues/337\n",
    "# 参考その2: https://github.com/slundberg/shap/issues/630\n",
    "import shap\n",
    "\n",
    "shap_values = []\n",
    "for model_ in models:\n",
    "    explainer = shap.TreeExplainer(model_)\n",
    "    shap_values.append(explainer.shap_values(test_feat_tge))\n",
    "    \n",
    "shap_mean = np.mean(shap_values, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_plot\n",
    "# 参考_画像の出力について: https://github.com/slundberg/shap/issues/153\n",
    "shap.summary_plot(shap_mean, test_feat_tge, show=False)\n",
    "plt.subplots_adjust(left=0.4, right=1.0)  # 保存画像のラベルが欠けるのを防ぐ\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'shap_summary_plot.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependence_plot\n",
    "# 参考1: https://github.com/slundberg/shap/issues/1206\n",
    "# 参考2: https://slundberg.github.io/shap/notebooks/plots/dependence_plot.html\n",
    "\n",
    "def dependence_plot_le(feat_disp:str):\n",
    "    # LEしたカテゴリ値を元に戻す\n",
    "    test_disp = test_feat_tge.copy()\n",
    "    feat_disp_prefix = 'LE_' + feat_disp\n",
    "    test_disp[feat_disp_prefix] = le_dic[feat_disp].inverse_transform(test_feat[feat_disp_prefix])\n",
    "\n",
    "    # 表示&保存\n",
    "    shap.dependence_plot(feat_disp_prefix, shap_mean, test_disp)\n",
    "    plt.subplots_adjust(left=0.4, right=1.0)  # 保存画像のラベルが欠けるのを防ぐ\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, f'shap_dependence_plot_{feat_disp}.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 全カテゴリで出すと重みに耐えられず死ぬので、保留\n",
    "for ctg in le_dic.keys():\n",
    "    dependence_plot_le(ctg)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dependence_plot_le('title_lang_ft')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "shap.dependence_plot('TGE_Mean_principal_maker', shap_mean, test_feat_tge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = atmacup10__sample_submission.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['likes'] = np.expm1(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# マイナス値は0とする\n",
    "sub['likes'] = sub['likes'].map(lambda x: 0 if x < 0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(os.path.join(OUTPUT_DIR, 'submission.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分布(train_vs_oof)\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "sns.distplot(train_target, label='Train', ax=ax, color='C1')\n",
    "sns.distplot(oof_preds, label='Out Of Fold', ax=ax, color='C2')\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'train_vs_oof.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分布(oof_vs_Test)\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "sns.distplot(oof_preds, label='Out Of Fold', ax=ax, color='C2')\n",
    "sns.distplot(np.log1p(test_preds), label='Test Predict', ax=ax, color='black')\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'oof_vs_test.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(oof_preds)[pd.Series(oof_preds) <= 1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
